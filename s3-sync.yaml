# this wil synchronise a shot to and from s3.

- hosts: ansible_control
  remote_user: vagrant
  become: true
  
  vars:
    bucket: "{{ show }}.{{ bucket_extension }}"
    volume_path: "{{ localnas1_mount_path }}"
    default_s3_push_args: --include "*"
    debug: false
    # these sync paths can be customised with your own by creating /secrets/vars/sync-paths.yaml and use this var as its contents
    sync_paths:
      - path: "{{ show }}/{{ seq }}/{{ shot }}/houdini"
        s3_push_args: --include "*.hip" --exclude "*backup/*" --exclude "*geo/*" --exclude "*pdgtemp/*"
      - path: "{{ show }}/{{ seq }}/{{ shot }}/assets"
    remote_push_paths:
      - path: "{{ show }}/{{ seq }}/{{ shot }}/render"
      - path: "{{ show }}/{{ seq }}/{{ shot }}/houdini"
        s3_push_args: --include "*.hip" --exclude "*backup/*" --exclude "*geo/*" --exclude "*pdgtemp/*"
      - path: "{{ show }}/{{ seq }}/{{ shot }}/assets"

  tasks:
  - name: Check for existance of custom sync paths and override default mounts
    stat:
      path:  ../../secrets/vars/sync-paths.yaml
    register: custom_sync_paths
    tags:
      - sync-local-push

  - name: Override default mounts
    include_vars:
      file:  ../../secrets/vars/sync-paths.yaml
    when: custom_sync_paths.stat.exists
    tags:
      - sync-local-push

  - name: Mount up prod with nfs
    mount:
      fstype: nfs4
      name: "{{ localnas1_mount_path }}"
      src: "{{ localnas1_private_ip }}:{{ localnas1_mount_path }}"
      path: "{{ localnas1_mount_path }}"
      opts: noatime
      state: mounted
    tags:
      - sync-local-push
      - mounts

  - name: ensure an s3 bucket exists
    s3_bucket:
      name: "{{ bucket }}"
      region: "{{ aws_region }}"
    tags:
      - sync-local-push

  - name: validate path existence
    stat:
      path: "{{ volume_path }}/{{ item.path }}"
    register: local_path_presence
    with_items:
      - "{{ sync_paths }}"
    tags:
      - sync-local-push

  - name: path data
    debug:
      var: item
    with_items: "{{ local_path_presence.results }}"
    when: debug
    tags:
      - sync-local-push

  - name: sync_paths existance check
    debug:
      msg: "path does not exist {{ item.item.path }}"
    become_user: vagrant
    when: not item.stat.exists
    with_items:
      - "{{ local_path_presence.results }}"
    tags:
      - sync-local-push

  - name: sync_paths
    shell: |
      set -x
      cd {{ volume_path }}/{{ item.item.path }}
      aws s3 sync . s3://{{ bucket }}/{{ item.item.path }} {{ item.item.s3_push_args | default( default_s3_push_args ) }}
    become_user: vagrant
    when: item.stat.exists
    with_items:
      - "{{ local_path_presence.results }}"
    tags:
      - sync-local-push

  - name: ensure dir exists when pulling
    file:
      path: "{{ volume_path }}/{{ item.path }}"
      state: directory
      owner: deadlineuser
      group: syscontrol
      mode: g+rw
    become: true
    with_items:
      - "{{ remote_push_paths }}"
    tags:
      - sync-local-pull

  - name: sync paths local pull
    shell: |
      export HISTCONTROL=ignorespace
        export AWS_ACCESS_KEY_ID={{ AWS_ACCESS_KEY }}
        export AWS_SECRET_ACCESS_KEY={{ AWS_SECRET_KEY }}
        export AWS_DEFAULT_REGION={{ aws_region }}
      set -x
      cd {{ volume_path }}/{{ item.path }}
      aws s3 sync s3://{{ bucket }}/{{ item.path }} . --include "*"
    become_user: vagrant
    with_items:
      - "{{ remote_push_paths }}"
    tags:
      - sync-local-pull

  - name: set permissions on paths from local pull recursively.
    file:
      dest: "{{ volume_path }}/{{ item.path }}"
      owner: deadlineuser
      group: syscontrol
      mode: u+rwX,g+rwX
      recurse: true 
    with_items:
      - "{{ remote_push_paths }}"
    tags:
      - sync-local-pull


      # consider aws s3 cp s3://bucket/large-object . &> out.log & tail -f out.log
      # https://github.com/aws/aws-cli/issues/2598

#after aws configure, you can sync with this example
# ansible-playbook -i "ansible/inventory" ansible/s3-sync.yaml --tags "sync-local-push" --extra-vars "volume_path=/prod"
# ansible-playbook -i "ansible/inventory" ansible/s3-sync.yaml --tags "sync-local-pull" --extra-vars "volume_path=/prod"
# ansible-playbook -i "ansible/inventory" ansible/s3-sync.yaml --tags "sync-remote-pull" --extra-vars "variable_user=centos variable_host=role_node_centos volume_path=/prod"
# ansible-playbook -i "ansible/inventory" ansible/s3-sync.yaml --tags "sync-remote-push" --extra-vars "volume_path=/pool0/volume0"
# ansible-playbook -i "ansible/inventory" ansible/s3-sync.yaml --tags "sync-remote-pull" --extra-vars "volume_path=/pool1/volume1"

- hosts: "{{ variable_host | default('role_softnas') }}"
  remote_user: "{{ variable_user | default('centos') }}"
  become: true

  vars:
    variable_user: centos
    pool_name: pool0
    volume_name: volume0

    bucket: "{{ show }}.{{ bucket_extension }}"

    volume_path: "/{{ pool_name }}/{{ volume_name }}"
    
    asset_path_nas: "{{ volume_path }}/{{ show }}/{{ seq }}/{{ shot }}/asset"
    asset_path_bucket: "s3://{{ bucket }}/{{ show }}/{{ seq }}/{{ shot }}/asset"

    shot_path_nas: "{{ volume_path }}/{{ show }}/{{ seq }}/{{ shot }}"
    shot_path_bucket: "s3://{{ bucket }}/{{ show }}/{{ seq }}/{{ shot }}"

    default_s3_push_args: --include "*"
    debug: false
    # these sync paths can be customised with your own by creating /secrets/vars/sync-paths.yaml and use this var as its contents
    sync_paths:
      - path: "{{ show }}/{{ seq }}/{{ shot }}/houdini"
        s3_push_args: --include "*.hip" --exclude "*backup/*" --exclude "*geo/*" --exclude "*pdgtemp/*"
      - path: "{{ show }}/{{ seq }}/{{ shot }}/assets"

  tasks:
  - name: Check for existance of custom sync paths and override default sync paths
    stat:
      path:  ../../secrets/vars/sync-paths.yaml
    register: custom_sync_paths
    connection: local
    tags:
      - sync-remote-pull
      - sync-remote-push

  - name: Override default paths
    include_vars:
      file:  ../../secrets/vars/sync-paths.yaml
    when: custom_sync_paths.stat.exists
    connection: local
    tags:
      - sync-remote-pull
      - sync-remote-push

  - name: ensure mounts are present
    shell: |
      mount -a
    become: true
    tags:
      - sync-remote-pull
      - sync-remote-push

  - name: set aws environment variables and test connection
    shell: |
      export HISTCONTROL=ignorespace
        export AWS_ACCESS_KEY_ID={{ AWS_ACCESS_KEY }}
        export AWS_SECRET_ACCESS_KEY={{ AWS_SECRET_KEY }}
        export AWS_DEFAULT_REGION={{ aws_region }}
      aws s3 ls
    become: true
    tags:
      - sync-remote-pull
      - sync-remote-push

  - name: ensure dir exists when pulling
    file:
      path: "{{ volume_path }}/{{ item.path }}"
      state: directory
      owner: deadlineuser
    become: true
    with_items:
      - "{{ sync_paths }}"
    tags:
      - sync-remote-pull

  - name: sync paths remote pull
    shell: |
      export HISTCONTROL=ignorespace
        export AWS_ACCESS_KEY_ID={{ AWS_ACCESS_KEY }}
        export AWS_SECRET_ACCESS_KEY={{ AWS_SECRET_KEY }}
        export AWS_DEFAULT_REGION={{ aws_region }}
      set -x
      cd {{ volume_path }}/{{ item.path }}
      aws s3 sync s3://{{ bucket }}/{{ item.path }} . --include "*"
    become_user: deadlineuser
    with_items:
      - "{{ sync_paths }}"
    tags:
      - sync-remote-pull

  - name: set permissions on paths after pull
    file:
      dest: "{{ volume_path }}/{{ item.path }}"
      owner: deadlineuser
      group: syscontrol
      mode: u+rwX,g+rwX
      recurse: true 
    with_items:
      - "{{ sync_paths }}"
    tags:
      - sync-remote-pull

  - name: validate path existence when pushing
    stat:
      path: "{{ volume_path }}/{{ item.path }}"
    register: remote_path_presence
    with_items:
      - "{{ remote_push_paths }}"
    tags:
      - sync-remote-push

  - name: path data
    debug:
      var: item
    with_items: "{{ remote_path_presence.results }}"
    when: debug
    tags:
      - sync-remote-push

  - name: sync_paths existance check
    debug:
      msg: "path does not exist {{ item.item.path }}"
    become_user: "{{ variable_user }}"
    when: not item.stat.exists
    with_items:
      - "{{ remote_path_presence.results }}"
    tags:
      - sync-remote-push

  - name: sync_paths remote push
    shell: |
      export HISTCONTROL=ignorespace
        export AWS_ACCESS_KEY_ID={{ AWS_ACCESS_KEY }}
        export AWS_SECRET_ACCESS_KEY={{ AWS_SECRET_KEY }}
        export AWS_DEFAULT_REGION={{ aws_region }}
      set -x
      cd {{ volume_path }}/{{ item.item.path }}
      aws s3 sync . s3://{{ bucket }}/{{ item.item.path }} {{ item.item.s3_push_args | default( default_s3_push_args ) }}
    become_user: "{{ variable_user }}"
    register: sync_remote_push_output
    when: item.stat.exists
    with_items:
      - "{{ remote_path_presence.results }}"
    tags:
      - sync-remote-push

  - name: output from remote push
    debug:
      var: item
    with_items:
      - "{{ sync_remote_push_output.results }}"
    tags:
      - sync-remote-push



# # aws command may need to be /home/{{ variable_user }}/.local/bin/
#   - name: sync asset files pull
#     shell: |
#       export HISTCONTROL=ignorespace
#         export AWS_ACCESS_KEY_ID={{ AWS_ACCESS_KEY }}
#         export AWS_SECRET_ACCESS_KEY={{ AWS_SECRET_KEY }}
#         export AWS_DEFAULT_REGION={{ aws_region }}
#       set -x
#       cd {{ asset_path_nas }}
#       aws s3 sync {{ shot_path_bucket }}/asset . --include "*"
#     become_user: "{{ variable_user }}"
#     tags:
#       - sync-remote-pull

#   - name: sync asset files push
#     shell: |
#       export HISTCONTROL=ignorespace
#         export AWS_ACCESS_KEY_ID={{ AWS_ACCESS_KEY }}
#         export AWS_SECRET_ACCESS_KEY={{ AWS_SECRET_KEY }}
#         export AWS_DEFAULT_REGION={{ aws_region }}
#       set -x
#       cd {{ asset_path_nas }}
#       aws s3 sync . {{ shot_path_bucket }}/asset --include "*"
#     become_user: "{{ variable_user }}"
#     tags:
#       - sync-remote-push

#   - name: ensure dir exists.
#     file:
#       path: "{{ shot_path_nas }}/cache"
#       state: directory
#       owner: "{{ variable_user }}"
#     become: true
#     tags:
#       - sync-remote-pull
#       - sync-remote-push

#   - name: sync cache files pull
#     shell: |
#       export HISTCONTROL=ignorespace
#         export AWS_ACCESS_KEY_ID={{ AWS_ACCESS_KEY }}
#         export AWS_SECRET_ACCESS_KEY={{ AWS_SECRET_KEY }}
#         export AWS_DEFAULT_REGION={{ aws_region }}
#       set -x
#       cd {{ shot_path_nas }}/cache
#       aws s3 sync {{ shot_path_bucket }}/cache . --include "*"
#     become_user: "{{ variable_user }}"
#     tags:
#       - sync-remote-pull

#   - name: sync cache files push
#     shell: |
#       export HISTCONTROL=ignorespace
#         export AWS_ACCESS_KEY_ID={{ AWS_ACCESS_KEY }}
#         export AWS_SECRET_ACCESS_KEY={{ AWS_SECRET_KEY }}
#         export AWS_DEFAULT_REGION={{ aws_region }}
#       set -x
#       cd {{ shot_path_nas }}/cache
#       aws s3 sync . {{ shot_path_bucket }}/cache --include "*"
#     become_user: "{{ variable_user }}"
#     tags:
#       - sync-remote-push

#   - name: ensure dir exists.
#     file:
#       path: "{{ shot_path_nas }}/houdini"
#       state: directory
#       owner: "{{ variable_user }}"
#     become: true
#     tags:
#       - sync-remote-pull
#       - sync-remote-push

#   - name: sync houdini files pull
#     shell: |
#       export HISTCONTROL=ignorespace
#         export AWS_ACCESS_KEY_ID={{ AWS_ACCESS_KEY }}
#         export AWS_SECRET_ACCESS_KEY={{ AWS_SECRET_KEY }}
#         export AWS_DEFAULT_REGION={{ aws_region }}
#       set -x
#       cd {{ shot_path_nas }}/houdini
#       aws s3 sync {{ shot_path_bucket }}/houdini . --include "*"
#     become_user: "{{ variable_user }}"
#     tags:
#       - sync-remote-pull

#   - name: sync houdini files push
#     shell: |
#       export HISTCONTROL=ignorespace
#         export AWS_ACCESS_KEY_ID={{ AWS_ACCESS_KEY }}
#         export AWS_SECRET_ACCESS_KEY={{ AWS_SECRET_KEY }}
#         export AWS_DEFAULT_REGION={{ aws_region }}
#       set -x
#       cd {{ shot_path_nas }}/houdini
#       aws s3 sync . {{ shot_path_bucket }}/houdini --include "*"
#     become_user: "{{ variable_user }}"
#     tags:
#       - sync-remote-push


#   - name: S3 GET objects
#     aws_s3:
#       bucket: "{{ bucket }}"
#       object: "{{ show }}/{{ seq }}/{{ shot }}/asset"
#       dest: "/{{ pool_name }}/{{ volume_name }}/"
#       mode: get
#       AWS_ACCESS_KEY: "{{ AWS_ACCESS_KEY }}"
#       AWS_SECRET_KEY: "{{ AWS_SECRET_KEY }}"
#       region: "{{ aws_region }}"