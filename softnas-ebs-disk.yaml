- hosts: ansible_control
  remote_user: vagrant

  vars:
    #  Creates ebs volumes.  example -
    #  ansible-playbook -i ansible/inventory ansible/softnas-ebs-disk.yaml -v --extra-vars "ebs_disk_size=200 instance_id=i-654654354"

    # you can use a custom description to add more mounts and ovveride the defaults here of 4 drives with
    # /vagrant/secrets/{{ envtier }}/ebs-volumes/softnas-ebs-volumes.yaml, where envtier is "dev" or "prod" for the current environment. and contents contain a copy of the mounts var below.

    # By default we start with a minimum of 4 volumes intended for a raid 6 array.
    # Raid 6 is used because its better than raid 5 with a hot spare (same number of drives), and allows double redundancy, which scale better to large volume counts.
    # Raid 6 is better than mirroring because it can scale, mirroring cannot.  We can append drives on the array (softnas say up to 20) to scale the volume up without interruption.

    # Consider your default ebs disk size for production.  Since it is not recommended to use more than 20 ebs volumes by Softnas, calculate the maximum space you think you will need if you scale up to that number.
    # eg, if I set the volume size to 1TB, then I will start by paying for 4 x 1TB volumes.  I can scale up to 20 x 1 TB, and considering raid 6 (-2 drives), we will have up to 18TB to scale to, and can suffer up two drive failures.

    # maintin a copy of all data as a backup.  Perhaps directly to s3, or using one of the default s3 drives, or onsite.
    
    # another example for idempotenccy.  good read!
    # https://medium.com/devopslinks/attaching-a-persistent-ebs-volume-to-a-self-healing-instance-with-ansible-d0140431a22a

    pool_name: pool2
    volume_name: volume2
    #instance_id: undefined
    disk_device: 2
    nas_name: softnas
    ebs_disk_size: 50
    # read docs on types.  standard, gp2 etc
    ebs_disk_type: standard
    ebs_disk_region: "{{ aws_region }}"
    import_pool: true
    # stopping the softnas instance ensures that any non present ebs volume pool will be available on boot.
    # this is not desirable if adding volumes to an existing pool that is in use.
    stop_softnas_instance: false

    # this will be calculated later-
    existing_bucket: false
    existing_volume: false

    mounts:
      - path: "/dev/sdf"
        int: "1"
      - path: "/dev/sdg"
        int: "2"
      - path: "/dev/sdh"
        int: "3"
      - path: "/dev/sdi"
        int: "4"

  tasks:
  - name: Check for existance of custom mounts in /vagrant/secrets/{{ envtier }}/ebs-volumes/softnas-ebs-volumes.yaml and override default mounts
    stat:
      path: ../secrets/{{ envtier }}/ebs-volumes/softnas-ebs-volumes.yaml
    register: custom_ebs_list

  - name: Override default mounts
    include_vars:
      file: ../secrets/{{ envtier }}/ebs-volumes/softnas-ebs-volumes.yaml
    when: custom_ebs_list.stat.exists

  - name: Search for existing softnas instance by tag.  If no instance id is defined via command line, then the found instance id by tag wil be used.
    ec2_instance_facts:
      filters:
        "tag:Role": "softnas"
      #region: "{{ ebs_disk_region }}"
    register: existing_softnas_instance

  - debug:
      msg: "{{ existing_softnas_instance.instances[0].instance_id }}"

  - set_fact:
      instance_id: "{{ existing_softnas_instance.instances[0].instance_id }}"
    when: instance_id is undefined

  - name: Stop the softnas instance before updating mounts.  This may take 5 minutes.
    ec2:
      instance_ids: '{{ instance_id }}'
      region: '{{ aws_region }}'
      state: stopped
      wait: True
    become_user: vagrant
    when: stop_softnas_instance

  - name: search for existing volume
    ec2_vol_facts:
      filters:
        "tag:Role": "Softnas"
        "tag:Mount": "{{ item.path }}"
      region: "{{ ebs_disk_region }}"
    register: existing_volume
    with_items:
      - "{{ mounts }}"

  - name: all volumes retrieved
    debug:
      msg: "{{ existing_volume }}"

  - name: iterate over data
    debug:
      msg: "{{ item }}"
    with_items: "{{ existing_volume.results }}"      

  - include_tasks: softnas-ebs-first-time-installation.yaml
    when: not item.volumes
    with_items: "{{ existing_volume.results }}"

  - include_tasks: softnas-ebs-existing-volume.yaml
    when: item.volumes
    with_items: "{{ existing_volume.results }}"

  - name: Start the softnas instance after updating mounts
    ec2:
      instance_ids: '{{ instance_id }}'
      region: '{{ aws_region }}'
      state: running
      wait: True
    become_user: vagrant
    when: stop_softnas_instance

# # update exports on softnas.
# - hosts: role_softnas
#   remote_user: centos
#   become_user: root
#   become: true

#   vars:
#     # when paths are found at these locations, they will be added to exports. otherwise no action will occur.
#     exports:
#       - path: "/pool1/volume1/"
#         pool_name: pool1
#         volume_name: volume1
#         state: present
#         bind: /prod

#     import_pool: true

#   tasks:
#   - name: Check for existance of custom exports in /vagrant/secrets/{{ envtier }}/ebs-volumes/softnas-ebs-volumes.yaml and override default mounts
#     stat:
#       path: ../secrets/{{ envtier }}/ebs-volumes/softnas-ebs-volumes.yaml
#     register: custom_ebs_list
#     connection: local

#   - name: Override default exports
#     include_vars:
#       file: ../secrets/{{ envtier }}/ebs-volumes/softnas-ebs-volumes.yaml
#     when: custom_ebs_list.stat.exists
#     connection: local

#   - name: set /etc/exports
#     lineinfile:
#       path: /etc/exports
#       line: "# These mounts are managed in ansible playbook softnas-s3-disk.yaml and only one volume is exported currently for testing"
#       backup: yes
#     when: import_pool

#   - name: ensure after a pool import that the path actually exists.  if so then exports will be updated
#     stat:
#       path: "/{{ item.pool_name }}/{{ item.volume_name }}/"
#     register: volume_mount
#     when: import_pool
#     with_items:
#       - "{{ exports }}"

#   - name: inspect dict of results
#     debug:
#       var: "{{ item }}"
#     with_items: "{{ volume_mount.results }}"
    
#   - name: check is dir
#     debug:
#       msg: "/{{ item.item.pool_name }}/{{ item.item.volume_name }}/ exists on softnas instance"
#     when: item.stat.exists and item.stat.isdir and import_pool
#     with_items: "{{ volume_mount.results }}"

#   - name: insert/update block in in /etc/exports for volume
#     blockinfile:
#       path: /etc/exports
#       block: |
#         /{{ item.item.pool_name }}/{{ item.item.volume_name }}/ *(async,insecure,no_subtree_check,no_root_squash,rw,nohide)
#       marker: "# {mark} ANSIBLE MANAGED BLOCK {{ item.item.path }}"
#       state: "{{ item.item.state }}"
#     when: item.stat.exists and item.stat.isdir and import_pool
#     with_items:
#       - "{{ volume_mount.results }}"
    
#   - name: update exports
#     command: "exportfs -r -a"
#     register: update_exports_output
#     become: true
#     when: import_pool

#   - debug:
#       msg: "{{ update_exports_output.stdout }}"
#     when: import_pool