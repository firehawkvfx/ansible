# ~/.bashrc

# The contents of config.template are modified by update_vars.sh.
# Editing these contents should only be done in secrets/config, and then propogated with 'source ./update_vars.sh'

# WARNING: When editing your config file do not store any secrets / sensitive information here.
# Secrets should only ever be stored in the encrypted secrets file..
# This unencrypted config file is still stored in your private repository and should not be publicly available.

# CONFIG INITIALIZATION #

# Private values must be used as values only (and not in #commented lines), since it is only the values that are kept private.
# Only Commented lines and the variable names / keys are read from this private secrets/config file in your private repo to auto generate the public firehawk/config.template file when running 'source ./update_vars.sh'.

# If these steps are followed then no private values will be or should be propogated into the public repo firehawk/config.template file.
# Before making any commits of config.template to the public firehawk repo ensure there are no secrets / sensitive information contained in a commit.
# Be sure to provide any new variable keys you may end up adding with a commented out description with example dummy values above your actual private value used to assist others.

# Do not put real world sensitive information in the example comments / #commented out lines.

# New comments should be only added in secrets/config as these lines will be propogated into the config.template schema used to initialise any new  secrets/config file for other users of the Firehawk repo.

# BEGIN CONFIGURATION #

# TF_VAR_gatewaynic:
# The gateway used from the vm used to access the network or internet.  type 'ip a' in the vagrant shell to see a list of possible NICs.
# eg: TF_VAR_gatewaynic=eth1
TF_VAR_gatewaynic=insertvalue

# TF_VAR_site_mounts:
# Enable site NFS mounts to be mounted on remote nodes.  If you have an existing local NFS share from your own NAS, it can be provided as a remote mount over vpn for cloud based nodes.  Operating with a local NFS share is requried for PDG/TOPS to function.
# default: TF_VAR_site_mounts=true
TF_VAR_site_mounts=insertvalue

# TF_VAR_remote_mounts_on_local:
# Enable cloud NFS mounts to be mounted on local nodes.  After the VPN is connected, the remote SoftNAS NFS shares can be mounted on the local workstation.  This is necesary for PDG/TOPS to track completed work items, and allow remote deletion of data.
# default: TF_VAR_remote_mounts_on_local=true
TF_VAR_remote_mounts_on_local=insertvalue

# TF_VAR_firehawk_sync_source:
# The location based path to the firehawk houdini tools repo.
# This location is pushed to S3, and when a softnas production volume is created it will be pulled to the cloud NFS share.
# It contains TOPS S3 sync functions which Side FX PDG will require to sync assets to and from S3.
# This should also be synced prior to rendering if any changes to assets occur.
# You should have a production NFS share /prod, ensure you also bind an identical locaiton based mount eg: /mycity_prod
# The data at these two paths is identical, but /mycity_prod allows access to data in another location over VPN without confusion, whereas /prod will always refer to the site based NAS for performance, and it is not garunteed to be identical to other locations.
# eg: TF_VAR_firehawk_sync_source=/cairns_prod/assets/openfirehawk-houdini-tools
TF_VAR_firehawk_sync_source=insertvalue

# TF_VAR_softnas_mode_dev:
# The ability to switch the performance of the softnas between [low/high] to save costs in dev environment.  
# Since this uses different AMI's for low and high, each allowing different instance ranges to be used, the settings should be identical for dev and prod environments.
# default: TF_VAR_softnas_mode_dev=high 
TF_VAR_softnas_mode_dev=insertvalue

# TF_VAR_softnas_mode_prod:
# the ability to switch the performance of the softnas between [low/high] to save costs in prod environment.
# default: TF_VAR_softnas_mode_prod=high 
TF_VAR_softnas_mode_prod=insertvalue

# TF_VAR_remote_subnet_cidr:
# This is the IP range (CIDR notation) of your subnet onsite that the firehawkserver vm will reside in, and that other onsite nodes reside in.
# eg: TF_VAR_remote_subnet_cidr=192.168.29.0/24
# This would be the ip range 192.168.29.0 - 192.168.29.255
TF_VAR_remote_subnet_cidr=insertvalue

# TF_VAR_workstation_address:
# The address of the workstation the artist will operate from onsite
# eg: TF_VAR_remote_subnet_cidr=192.168.92.31
TF_VAR_workstation_address=insertvalue

# TF_VAR_houdini_license_server_address:
# The ip of the houdini licence server used when in a dev or prod environment.  This will normally be the same ip server used in dev and production to not waste licences. So in a dev environment, you will probably need to reference the same houdini licence server in production.  Ideally, the houdini license server should be a seperate vm to the vagrant host for stability reasons but this is currently untested.  It is recomended that the licence server ip is the vagrant production VM until otherwise tested.  A licence server should rarely need to be touched, but updates to infrastructure could disrupt it if located on the firehawkserver vm.
# eg: TF_VAR_houdini_license_server_address=192.168.29.80
TF_VAR_houdini_license_server_address=insertvalue

# TF_VAR_localnas1_private_ip:
# The IP adress of the onsite system with NFS shares to mount.  This is normally your onsite NAS ip.
# eg: TF_VAR_localnas1_private_ip=192.168.29.11
TF_VAR_localnas1_private_ip=insertvalue

# TF_VAR_localnas1_mount_path:
# The name of the production volume path for onsite workstations/nodes.  /prod is a relative path depending on location, so for cloud based nodes, this would usually refer to the softnas production path.  So far only /prod has been tested.
# default: TF_VAR_localnas1_mount_path=/prod
TF_VAR_localnas1_mount_path=insertvalue

# TF_VAR_localnas1_remote_mount_path:
# The name of the offsite production path if mounted over vpn for onsite workstations/nodes
# default: TF_VAR_localnas1_remote_mount_path=/prod_remote
TF_VAR_localnas1_remote_mount_path=insertvalue

# TF_VAR_localnas1_path_abs:
# This is the absolute path for the onsite NFS mount at all locations. For example, /mycity_prod should be the same mount path for all locations over vpn.
# eg: TF_VAR_localnas1_path_abs=/mycity_prod
TF_VAR_localnas1_path_abs=insertvalue

# TF_VAR_softnas1_path_abs:
# This is the absolute path for the cloud NFS mount at all locations. For example, /aws_city_prod should be the same mount path for all locations over VPN.
# eg: TF_VAR_softnas1_path_abs=/aws_city_prod
TF_VAR_softnas1_path_abs=insertvalue

# TF_VAR_firehawk_houdini_tools:
# The path used in produciton to reference firehawk houdini tools for all locations.  This should normally be in /prod, which is the most performant site based NFS mount for any location.
# default: TF_VAR_firehawk_houdini_tools=/prod/assets/openfirehawk-houdini-tools
TF_VAR_firehawk_houdini_tools=insertvalue

# TF_VAR_deadline_version:
# The version of the deadline installer.  Upgrading from 10.0.25.2 has changes to the way the spot event plugin is updated in the DB.  currently, it is a dictionary with key/value pairs, but in more recent versions of deadline this is now a list of lists with key value pairs.
# default: TF_VAR_deadline_version=10.1.1.3
TF_VAR_deadline_version=insertvalue

# TF_VAR_deadline_proxy_root_dir_dev:
# This is the address and port for clients to reach the Deadline RCS on the openfirehawk server in a dev environment.
# eg: TF_VAR_deadline_proxy_root_dir_dev=192.168.29.10:4433
TF_VAR_deadline_proxy_root_dir_dev=insertvalue

# TF_VAR_deadline_proxy_root_dir_prod:
# This is the address and port for clients to reach the Deadline Remote Connection Server on the openfirehawk server in a prod environment.
# eg: TF_VAR_deadline_proxy_root_dir_prod=192.168.29.80:4433
TF_VAR_deadline_proxy_root_dir_prod=insertvalue

# TF_VAR_ebs_disk_size_prod:
# The size of each EBS disk used on the softnas instance in the production environment.  multiple drives can be used to scale an array up to 20 disks if using Raid 5/6.
# default: TF_VAR_ebs_disk_size_prod=1000
TF_VAR_ebs_disk_size_prod=insertvalue

# TF_VAR_ebs_disk_size_dev:
# The size of each EBS disk used on the softnas instance in the dev environment.  multiple drives can be used to scale an array up to 20 disks if using Raid 5/6.
# default: TF_VAR_ebs_disk_size_dev=200
TF_VAR_ebs_disk_size_dev=insertvalue

# TF_VAR_workstation_enabled:
# terraform and ansible will provision a cloud based workstation if true.
# default: TF_VAR_workstation_enabled=false
TF_VAR_workstation_enabled=insertvalue

# TF_VAR_provision_deadline_spot_plugin:
# If enabled, the deadline spot fleet plugin will be automatically configured.  
# Note that because we alter the mongo db, this may not be supported with future versions of deadline.  You may need to disable it and configure the plugin manually in these circumstances.
# default: TF_VAR_provision_deadline_spot_plugin=true
TF_VAR_provision_deadline_spot_plugin=insertvalue
