# ~/.bashrc

# The contents of config.template are modified by update_vars.sh.
# Editing these contents should only be done in secrets/config, and then propogated with 'source ./update_vars.sh'

# WARNING: When editing your config file do not store any secrets / sensitive information here.
# Secrets should only ever be stored in the encrypted secrets file..
# This unencrypted config file is still stored in your private repository and should not be publicly available.

# CONFIG INITIALIZATION #

# Private values must be used as values only (and not in #commented lines), since it is only the values that are kept private.
# Only Commented lines and the variable names / keys are read from this private secrets/config file in your private repo to auto generate the public firehawk/config.template file when running 'source ./update_vars.sh'.

# If these steps are followed then no private values will be or should be propogated into the public repo firehawk/config.template file.
# Before making any commits of config.template to the public firehawk repo ensure there are no secrets / sensitive information contained in a commit.
# Be sure to provide any new variable keys you may end up adding with a commented out description with example dummy values above your actual private value used to assist others.

# Do not put real world sensitive information in the example comments / #commented out lines.

# New comments should be only added in secrets/config as these lines will be propogated into the config.template schema used to initialise any new  secrets/config file for other users of the Firehawk repo.

# BEGIN CONFIGURATION #

# TF_VAR_gatewaynic:
# The gateway used from the vm used to access the network or internet.  type 'ip a' in the vagrant shell to see a list of possible NICs.
# eg: TF_VAR_gatewaynic=eth1
TF_VAR_gatewaynic=insertvalue

# TF_VAR_inventory_dev: 
# The relative path to the inventory maintained for ansible from the /vagrant path.
# This should exist outside the public repository and the default should be used.
# default: TF_VAR_inventory_dev=../secrets/dev/inventory
TF_VAR_inventory_dev=insertvalue

# TF_VAR_inventory_prod:
# The relative path to the inventory maintained for ansible from the /vagrant path.
# This should exist outside the public repository and the default should be used.
# default: TF_VAR_inventory_prod=../secrets/prod/inventory
TF_VAR_inventory_prod=insertvalue

# TF_VAR_softnas_storage:
# Enable softnas storage.  Softnas is used for a centralised cloud based NFS share.
# It's licenceing cost scales with usage, and can provide RAID redundancy.
# default: TF_VAR_softnas_storage=true
TF_VAR_softnas_storage=insertvalue

# TF_VAR_site_mounts:
# Enable site NFS mounts to be mounted on remote nodes.  If you have an existing local NFS share from your own NAS, it can be provided as a remote mount over vpn for cloud based nodes.  Operating with a local NFS share is requried for PDG/TOPS to function.
# default: TF_VAR_site_mounts=true
TF_VAR_site_mounts=insertvalue

# TF_VAR_remote_mounts_on_local:
# Enable cloud NFS mounts to be mounted on local nodes.  After the VPN is connected, the remote SoftNAS NFS shares can be mounted on the local workstation.  This is necesary for PDG/TOPS to track completed work items, and allow remote deletion of data.
# default: TF_VAR_remote_mounts_on_local=true
TF_VAR_remote_mounts_on_local=insertvalue

# TF_VAR_private_subnet1_prod:
# The production IP range for private subnet 1 for workers.  This subnet is accessed via VPN or bastion hosts.
# default: TF_VAR_private_subnet1_prod=10.0.1.0/24
TF_VAR_private_subnet1_prod=insertvalue

# TF_VAR_private_subnet2_prod:
# The production IP range for private subnet 2 for workers.  This subnet is accessed via VPN or bastion hosts.
# default: TF_VAR_private_subnet2_prod=10.0.2.0/24
TF_VAR_private_subnet2_prod=insertvalue

# TF_VAR_public_subnet1_prod:
# The production IP range for public subnet 1 for public facing  systems.  examples are your VPN access server instance, or bastion host for provisioning other instances in the private network.
# default: TF_VAR_public_subnet1_prod=10.0.101.0/24
TF_VAR_public_subnet1_prod=insertvalue

# TF_VAR_public_subnet2_prod:
# The production IP range for public subnet 2 for public facing systems.  examples are your VPN access server instance, or bastion host for provisioning other instances in the private network.
# default: TF_VAR_public_subnet2_prod=10.0.102.0/24
TF_VAR_public_subnet2_prod=insertvalue

# TF_VAR_private_subnet1_dev:
# The dev IP range for private subnet 1 for workers.  This subnet is accessed via VPN or bastion hosts.
# default: TF_VAR_private_subnet1_dev=10.0.11.0/24
TF_VAR_private_subnet1_dev=insertvalue

# TF_VAR_private_subnet2_dev:
# The dev IP range for private subnet 2 for workers.  This subnet is accessed via VPN or bastion hosts.
# default: TF_VAR_private_subnet2_dev=10.0.12.0/24
TF_VAR_private_subnet2_dev=insertvalue

# TF_VAR_public_subnet1_dev:
# The dev IP range for public subnet 1 for public facing  systems.  examples are your VPN access server instance, or bastion host for provisioning other instances in the private network.
# default: TF_VAR_public_subnet1_dev=10.0.201.0/24
TF_VAR_public_subnet1_dev=insertvalue

# TF_VAR_public_subnet2_dev:
# The dev IP range for public subnet 2 for public facing systems.  examples are your VPN access server instance, or bastion host for provisioning other instances in the private network.
# default: TF_VAR_public_subnet2_dev=10.0.202.0/24
TF_VAR_public_subnet2_dev=insertvalue

# TF_VAR_firehawk_sync_source:
# The location based path to the firehawk houdini tools repo.
# This location is pushed to S3, and when a softnas production volume is created it will be pulled to the cloud NFS share.
# It contains TOPS S3 sync functions which Side FX PDG will require to sync assets to and from S3.
# This should also be synced prior to rendering if any changes to assets occur.
# You should have a production NFS share /prod, ensure you also bind an identical locaiton based mount eg: /mycity_prod
# The data at these two paths is identical, but /mycity_prod allows access to data in another location over VPN without confusion, whereas /prod will always refer to the site based NAS for performance, and it is not garunteed to be identical to other locations.
# eg: TF_VAR_firehawk_sync_source=/cairns_prod/assets/openfirehawk-houdini-tools
TF_VAR_firehawk_sync_source=insertvalue

# TF_VAR_softnas_mode_dev:
# The ability to switch the performance of the softnas between [low/high] to save costs in dev environment.  
# Since this uses different AMI's for low and high, each allowing different instance ranges to be used, the settings should be identical for dev and prod environments.
# default: TF_VAR_softnas_mode_dev=high 
TF_VAR_softnas_mode_dev=insertvalue

# TF_VAR_softnas_mode_prod:
# the ability to switch the performance of the softnas between [low/high] to save costs in prod environment.
# default: TF_VAR_softnas_mode_prod=high 
TF_VAR_softnas_mode_prod=insertvalue

# TF_VAR_remote_subnet_cidr:
# This is the IP range (CIDR notation) of your subnet onsite that the firehawkserver vm will reside in, and that other onsite nodes reside in.
# eg: TF_VAR_remote_subnet_cidr=192.168.29.0/24
# This would be the ip range 192.168.29.0 - 192.168.29.255
TF_VAR_remote_subnet_cidr=insertvalue

# TF_VAR_vpn_cidr_prod:
#  Open VPN sets up DHCP in this range for every connection in the dev env to provide a unique ip on each side of the VPN for every system. Dont change this from the default for now. In the future it should be unique. Reference for potential ranges https://www.arin.net/reference/research/statistics/address_filters/
# default: TF_VAR_vpn_cidr_prod=172.27.232.0/24
TF_VAR_vpn_cidr_prod=insertvalue

# TF_VAR_vpn_cidr_dev:
#  Open VPN sets up DHCP in this range for every connection in the dev env to provide a unique ip on each side of the VPN for every system. Dont change this from the default for now. In the future it should be unique. Reference for potential ranges https://www.arin.net/reference/research/statistics/address_filters/
# default: TF_VAR_vpn_cidr_dev=172.17.232.0/24
TF_VAR_vpn_cidr_dev=insertvalue

# TF_VAR_houdini_license_server_address_dev:
# The ip of the houdini licence server used when in a dev environment.  This will normally be the same ip server used in dev and production to not waste licences. So in a dev environment, you will probably need to reference the same houdini licence server in production.  Ideally, the houdini license server should be a seperate vm to the vagrant host for stability reasons but this is currently untested.  It is recomended that the licence server ip is the vagrant production VM until otherwise tested.  A licence server should rarely need to be touched, but updates to infrastructure could disrupt it if located on the firehawkserver vm.
# eg: TF_VAR_houdini_license_server_address_dev=192.168.29.80
TF_VAR_houdini_license_server_address_dev=insertvalue

# TF_VAR_houdini_license_server_address_prod:
# The ip of the houdini licence server used when in a prod environment.  This will normally be the same ip server used in dev and production to not waste licences. So in a dev environment, you will probably need to reference the same houdini licence server in production.  Ideally, the houdini license server should be a seperate vm to the vagrant host for stability reasons but this is currently untested.  It is recomended that the licence server ip is the vagrant production VM until otherwise tested.  A licence server should rarely need to be touched, but updates to infrastructure could disrupt it if located on the firehawkserver vm.
# eg: TF_VAR_houdini_license_server_address_prod=192.168.29.80
TF_VAR_houdini_license_server_address_prod=insertvalue

# TF_VAR_localnas1_private_ip:
# The IP adress of the onsite system with NFS shares to mount.  This is normally your onsite NAS ip.
# eg: TF_VAR_localnas1_private_ip=192.168.29.11
TF_VAR_localnas1_private_ip=insertvalue

# TF_VAR_localnas1_mount_path:
# The name of the production volume path for onsite workstations/nodes.  /prod is a relative path depending on location, so for cloud based nodes, this would usually refer to the softnas production path.  So far only /prod has been tested.
# default: TF_VAR_localnas1_mount_path=/prod
TF_VAR_localnas1_mount_path=insertvalue

# TF_VAR_localnas1_remote_mount_path:
# The name of the offsite production path if mounted over vpn for onsite workstations/nodes
# default: TF_VAR_localnas1_remote_mount_path=/prod_remote
TF_VAR_localnas1_remote_mount_path=insertvalue

# TF_VAR_localnas1_path_abs:
# This is the absolute path for the onsite NFS mount at all locations. For example, /mycity_prod should be the same mount path for all locations over vpn.
# eg: TF_VAR_localnas1_path_abs=/mycity_prod
TF_VAR_localnas1_path_abs=insertvalue

# TF_VAR_softnas1_path_abs:
# This is the absolute path for the cloud NFS mount at all locations. For example, /aws_city_prod should be the same mount path for all locations over VPN.
# eg: TF_VAR_softnas1_path_abs=/aws_city_prod
TF_VAR_softnas1_path_abs=insertvalue

# TF_VAR_firehawk_houdini_tools:
# The path used in produciton to reference firehawk houdini tools for all locations.  This should normally be in /prod, which is the most performant site based NFS mount for any location.
# default: TF_VAR_firehawk_houdini_tools=/prod/assets/openfirehawk-houdini-tools
TF_VAR_firehawk_houdini_tools=insertvalue

# TF_VAR_openfirehawkserver_name_dev:
# The hostname for the firehawkserver in the dev environment.  consider using the Fully Qualified Domain Name for the host as below.  This domain extension may be different for the dev environment to isolate the domain from production and allow AWS to control it via Route 53.
# eg: TF_VAR_openfirehawkserver_name_dev=openfirehawkserverdev.devexample.com
TF_VAR_openfirehawkserver_name_dev=insertvalue

# TF_VAR_openfirehawkserver_name_prod:
# The hostname for the firehawkserver in the prod environment.  consider using the Fully Qualified Domain Name for the host as below.  This domain extension may be different for the dev environment to isolate the domain from production and allow AWS to control it via Route 53.
# eg: TF_VAR_openfirehawkserver_name_prod=openfirehawkserver.prodexample.com
TF_VAR_openfirehawkserver_name_prod=insertvalue

# TF_VAR_deadline_version:
# The version of the deadline installer.  Upgrading from 10.0.25.2 has changes to the way the spot event plugin is updated in the DB.  currently, it is a dictionary with key/value pairs, but in more recent versions of deadline this is now a list of lists with key value pairs.
# default: TF_VAR_deadline_version=10.1.1.3
TF_VAR_deadline_version=insertvalue

# TF_VAR_deadline_proxy_root_dir_dev:
# This is the address and port for clients to reach the Deadline RCS on the openfirehawk server in a dev environment.
# eg: TF_VAR_deadline_proxy_root_dir_dev=192.168.29.10:4433
TF_VAR_deadline_proxy_root_dir_dev=insertvalue

# TF_VAR_deadline_proxy_root_dir_prod:
# This is the address and port for clients to reach the Deadline Remote Connection Server on the openfirehawk server in a dev environment.
# eg: TF_VAR_deadline_proxy_root_dir_prod=192.168.29.80:4433
TF_VAR_deadline_proxy_root_dir_prod=insertvalue

# TF_VAR_s3_disk_size_prod:
# Default s3 disk size in GB for the prod environment. This is thin provisioned and will not result in charges unless used so you should set it to the largest size you think you might need for output data.
# default: TF_VAR_s3_disk_size_prod=10000
TF_VAR_s3_disk_size_prod=insertvalue

# TF_VAR_s3_disk_size_dev:
# Default s3 disk size in GB for the dev environment. This is thin provisioned and will not result in charges unless used so you should set it to the largest size you think you might need for output data.
# default: TF_VAR_s3_disk_size_dev=10000
TF_VAR_s3_disk_size_dev=insertvalue

# TF_VAR_ebs_disk_size_prod:
# The size of each EBS disk used on the softnas instance in the production environment.  multiple drives can be used to scale an array up to 20 disks if using Raid 5/6.
# default: TF_VAR_ebs_disk_size_prod=1000
TF_VAR_ebs_disk_size_prod=insertvalue

# TF_VAR_ebs_disk_size_dev:
# The size of each EBS disk used on the softnas instance in the dev environment.  multiple drives can be used to scale an array up to 20 disks if using Raid 5/6.
# default: TF_VAR_ebs_disk_size_dev=200
TF_VAR_ebs_disk_size_dev=insertvalue

# TF_VAR_softnas1_private_ip1_dev:
# The private ip to install softnas dev with for its first interface.
# When using softnas with high availability and failover, a 2nd interface is used for replication to a 2nd softnas instance.
# default: TF_VAR_softnas1_private_ip1_dev=10.0.11.11
TF_VAR_softnas1_private_ip1_dev=insertvalue

# TF_VAR_softnas1_private_ip2_dev:
# The private ip to install softnas dev with for its 2nd interface.
# When using softnas with high availability and failover, this 2nd interface is used for replication to a 2nd softnas instance.
# default: TF_VAR_softnas1_private_ip2_dev=10.0.11.12
TF_VAR_softnas1_private_ip2_dev=insertvalue

# TF_VAR_softnas2_private_ip1_dev:
# The private ip to install softnas dev with for its first interface.
# When using softnas with high availability and failover, a 2nd interface is used for replication to a 2nd softnas instance.
# default: TF_VAR_softnas2_private_ip1_dev=10.0.11.11
TF_VAR_softnas2_private_ip1_dev=insertvalue

# TF_VAR_softnas2_private_ip2_dev:
# The private ip to install softnas dev with for its 2nd interface.
# When using softnas with high availability and failover, this 2nd interface is used for replication to a 2nd softnas instance.
# default: TF_VAR_softnas2_private_ip2_dev=10.0.11.12
TF_VAR_softnas2_private_ip2_dev=insertvalue

# TF_VAR_softnas1_private_ip1_prod:
# The private ip to install softnas prod with for its first interface.
# When using softnas with high availability and failover, a 2nd interface is used for replication to a 2nd softnas instance.
# default: TF_VAR_softnas1_private_ip1_prod=10.0.1.11
TF_VAR_softnas1_private_ip1_prod=insertvalue

# TF_VAR_softnas1_private_ip2_prod:
# The private ip to install softnas prod with for its 2nd interface.
# When using softnas with high availability and failover, this 2nd interface is used for replication to a 2nd softnas instance.
# default: TF_VAR_softnas1_private_ip2_prod=10.0.1.12
TF_VAR_softnas1_private_ip2_prod=insertvalue

# TF_VAR_softnas2_private_ip1_prod:
# The private ip to install softnas prod with for its first interface.
# When using softnas with high availability and failover, a 2nd interface is used for replication to a 2nd softnas instance.
# default: TF_VAR_softnas2_private_ip1_prod=10.0.1.11
TF_VAR_softnas2_private_ip1_prod=insertvalue

# TF_VAR_softnas2_private_ip2_prod:
# The private ip to install softnas prod with for its 2nd interface.
# When using softnas with high availability and failover, this 2nd interface is used for replication to a 2nd softnas instance.
# default: TF_VAR_softnas2_private_ip2_prod=10.0.1.12
TF_VAR_softnas2_private_ip2_prod=insertvalue

# ANSIBLE_FORCE_COLOR:
# When ansible is run via terraform local exec, colour is not normally visible.  This setting enforces color.
# default: ANSIBLE_FORCE_COLOR=1
ANSIBLE_FORCE_COLOR=insertvalue

# TF_VAR_workstation_enabled:
# terraform and ansible will provision a cloud based workstation if true.
TF_VAR_workstation_enabled=insertvalue

# TF_VAR_node_centos_instance_type:
# The AWS instance type for the node used to provision the spot fleet image from.
# Note this is only used for provisioning the AMI.  The actual instance types used for rendering are controlled by the spot fleet template, and can vary.
# Usually it makes sense to provision this instance with a low cost instance type, but when testing workloads you may wish to increase it.
# https://aws.amazon.com/ec2/pricing/on-demand/
TF_VAR_node_centos_instance_type=insertvalue

# TF_VAR_provision_deadline_spot_plugin:
# If enabled, the deadline spot fleet plugin will be automatically configured.  
# Note that because we alter the mongo db, this may not be supported with future versions of deadline.  You may need to disable it and configure the plugin manually in these circumstances.
TF_VAR_provision_deadline_spot_plugin=insertvalue
