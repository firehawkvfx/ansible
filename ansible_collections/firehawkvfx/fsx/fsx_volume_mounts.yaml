# This playbook will validate exports on fsx and mount those with the /etc/fstab file.

# to update a workstation after altering any fsx mounts for the first time use the command -
# ansible-playbook -i ansible/inventory ansible/ansible_collections/firehawkvfx/fsx/fsx_volume_mounts.yaml --extra-vars "variable_host=role_workstation_centos hostname=cloud_workstation1.firehawkvfx.com"

# to update a render node, just use -
# ansible-playbook -i ansible/inventory ansible/ansible_collections/firehawkvfx/fsx/fsx_volume_mounts.yaml

# to update an onsite centos workstation use-
# ansible-playbook -i "$TF_VAR_inventory" ansible/ansible_collections/firehawkvfx/fsx/fsx_volume_mounts.yaml -v -v --extra-vars "variable_host=workstation1 variable_user=deployuser hostname=workstation1 ansible_ssh_private_key_file=$TF_VAR_onsite_workstation_private_ssh_key" --skip-tags 'cloud_install'

# to configure onsite mounts only on the ansible control, skipping fsx export check
# ansible-playbook -i "$TF_VAR_inventory" ansible/ansible_collections/firehawkvfx/fsx/fsx_volume_mounts.yaml -v -v --extra-vars "variable_host=localhost variable_user=deployuser fsx_hosts=none" --tags 'local_install_onsite_mounts'

- hosts: "{{ variable_host | default('role_node_centos') }}"
  remote_user: "{{ variable_user | default('centos') }}"
  gather_facts: "{{ variable_gather_facts | default('false') }}"
  become: true
  vars_files:
    - /deployuser/ansible/group_vars/all/vars
    - vars/main.yml

  vars:
    destroy: false
    import_pool: true
    variable_user: centos
    secrets_path: "{{ lookup('env','TF_VAR_secrets_path') }}"
    fsx_ip: # this should be provided inline since it is unique on each tf apply
    # ansible_become_pass: "{{ user_deadlineuser_pw }}"
    # fsx_exports: "{{ hostvars[groups['role_node_centos'][0]]['presence']['results'] }}"
    pcoip: false
    destroy: false
    import_pool: true
    vars_files_locs: [ "/{{ secrets_path }}/{{ lookup('env','TF_VAR_envtier') }}/fsx_volumes/fsx_volumes.yaml", "files/fsx_volumes_{{ lookup('env','TF_VAR_envtier') }}.yaml", "files/fsx_volumes.yaml" ] # The first file found will be used, allowing the one in your secrets location to override defaults.

  pre_tasks:
  - name: test connection and permissions
    debug:
      msg: "connection established"
    tags:
    - local_install
    - cloud_install

  - name: aquire vars from secrets path before using defaults for fsx mounts
    include_vars: "{{ item }}"
    with_first_found: "{{ vars_files_locs }}"
    tags:
    - always

  - name: exports
    debug:
      var: item
    with_items: "{{ exports }}"
    when: destroy == false
    tags:
    - local_install
    - cloud_install

  - name: exports
    package:
      name: jq
    tags:
    - always

  - name: Check whether /etc/exports contains the mount
    # command: grep -E "^\/export\/{{ item.pool_name }}\/{{ item.volume_name }}.*" /etc/exports
    shell: |
      # aws fsx describe-file-systems | jq ".FileSystems[] | select(.LustreConfiguration.MountName == \"{{ item.volume_name }}\")"
      aws fsx describe-file-systems | jq --slurp ".[0].FileSystems[] | select(.LustreConfiguration.MountName == \"{{ item.volume_name }}\")"
    args:
      executable: /bin/bash
    register: presence
    check_mode: no
    ignore_errors: yes
    changed_when: no
    with_items: "{{ exports }}"
    when: destroy == false
    tags:
    - local_install
    - cloud_install

  - name: export existance test
    debug:
      var: item
    when: destroy == false and item.rc == 0
    with_items: "{{ presence.results }}"
    tags:
    - local_install
    - cloud_install

  - name: export output always
    set_fact: fsx_exports="{{ presence.results }}"
    when: destroy == false
    tags:
    - local_install
    - cloud_install

  # - name: export output always
  #   set_fact: exported_fsx_mounts="{{item}}"
  #   with_items: "{{ presence.results }}"
  #   when: destroy == false
  #   tags:
  #   - local_install
  #   - cloud_install

# # update fstab with valid mounts

# - hosts: "{{ variable_host | default('role_node_centos') }}"
#   remote_user: "{{ variable_user | default('centos') }}"
#   gather_facts: "{{ variable_gather_facts | default('false') }}"
#   become: true
#   vars_files:
#     - /deployuser/ansible/group_vars/all/vars
#     - vars/main.yml

  - name: exports
    debug:
      var: item
    with_items: "{{ exports }}"
    when: destroy | bool
    tags:
    - local_install
    - cloud_install

  - name: fsx exports to mount to this instance
    debug:
      var: item
    with_items: 
    - "{{ fsx_exports }}"
    tags:
    - local_install
    - cloud_install

  - name: create mount directories
    file: 
      path: "{{ item.item.path }}"
      state: directory
      owner: "{{ variable_user }}"
      group: "{{ variable_user }}"
    become: true
    when: destroy == false and item.rc == 0
    with_items: 
    - "{{ fsx_exports }}"
    tags:
    - local_install
    - cloud_install

  - name: create bind1 directories
    file: 
      path: "{{ item.item.bind1 }}"
      state: directory
      owner: "{{ variable_user }}"
      group: "{{ variable_user }}"
    when: destroy == false and item.item.bind1 and item.rc == 0
    with_items: 
    - "{{ fsx_exports }}"
    tags:
    - local_install
    - cloud_install

  - name: create bind2 directories
    file: 
      path: "{{ item.item.bind2 }}"
      state: directory
      owner: "{{ variable_user }}"
      group: "{{ variable_user }}"
    when: destroy == false and item.item.bind2 and item.rc == 0
    with_items: 
    - "{{ fsx_exports }}"
    tags:
    - local_install
    - cloud_install

  - fail:
      msg: "{{ item.item.path }} is set to be present in exports dict but doesn't exist in /etc/exports"
    when: destroy == false and item.item.state == "present" and item.rc == 1
    with_items:
    - "{{ fsx_exports }}"
    tags:
    - local_install
    - cloud_install

  - debug:
      msg: 'local onsite install will use different mounts'
    tags:
    - local_install

  - name: mount fsx Lustre volume from export  - master
    become: yes
    mount:
      fstype: lustre
      path: "/{{ item.item.volume_name }}"
      opts: defaults,noatime,flock,_netdev 0 0
      # source must pull the ip from the fsx primary interface
      src: "{{ fsx_ip }}:tcp/{{ item.item.volume_name }}"
      state: "{{ ( destroy == false and item.item.path and item.rc == 0 and item.item.state == 'present') | ternary( 'mounted' , 'absent' ) }}"
    when: destroy == false
    with_items: "{{ fsx_exports }}"
    tags:
    - local_install

  - name: exports check
    debug:
      var: item.bind2
    with_items: "{{ exports }}"
    when: destroy | bool
    tags:
    - local_install
    - cloud_install

  - name: bind master mounts to named paths.  bind2 references the absolute mount names such as /cloud_prod.  bind1 is relative site names such as /prod, which are not pushed from cloud to onsite since those paths should exist onsite from a high performance local mount
    become: yes
    mount:
      fstype: none
      path: "{{ item.item.bind2 }}"
      opts: "x-systemd.requires=/{{ item.item.volume_name }},x-systemd.automount,bind,_netdev"
      src: "/{{ item.item.volume_name }}"
      # if the path exists, and it was found in the exports, then set to mounted, else remove.
      state: "{{ ( destroy == false and item.item.path and item.item.bind2 and item.rc == 0 and item.item.state == 'present' ) | ternary( 'mounted' , 'absent' ) }}"
    when: destroy == false
    with_items: "{{ fsx_exports }}"
    tags:
    - local_install

### Force removal of bind2 mounts when destroy is true.  currently the ansible command to unmount will hang if the connetion has been broken.
  - debug:
      var: item
    when: destroy | bool
    with_items: "{{ exports }}"
    tags:
    - local_install

  - name: check if bind2 mount points exist for forced removal
    command: timeout 5 mountpoint -q {{ item.bind2 }}
    become: yes
    register: volume_stat_bind2
    failed_when: False
    changed_when: False
    when: destroy | bool
    with_items: "{{ exports }}"
    tags:
    - local_install

  - debug:
      var: item
    when: destroy | bool
    with_items: "{{ volume_stat_bind2.results }}"
    tags:
    - local_install

  - name: report mount points
    debug:
      msg: "This is a mountpoint that will be removed"
    when: destroy | bool and ( item.rc == 0 or item.rc == 124 )
    with_items: "{{ volume_stat_bind2.results }}"
    tags:
    - local_install

  - name: force unmount with shell when destroy is true.  bind must be removed before the hard mount - master.
    become: yes
    shell: |
      umount -l {{ item.item.bind2 }}
    when: destroy and ( item.rc == 0 or item.rc == 124 )
    with_items: "{{ volume_stat_bind2.results }}"
    tags:
    - local_install

### End force removal of mounts

  - name: unmount when destroy is true.  bind must be removed before the hard mount - master.
    become: yes
    mount:
      path: "{{ item.bind2 }}"
      state: unmounted
    when: destroy | bool
    with_items: "{{ exports }}"
    tags:
    - local_install

  - name: remove mounts from fstab when destroy is true.  bind must be removed before the hard mount - master.
    become: yes
    mount:
      path: "{{ item.bind2 }}"
      state: absent
    when: destroy | bool
    with_items: "{{ exports }}"
    tags:
    - local_install

### Force removal of master mounts.  currently the ansible command to unmount will hang if the connetion has been broken.

  - name: check if master mount points exist for forced removal
    command: timeout 5 mountpoint -q {{ item.path }}
    become: yes
    register: volume_stat_path
    failed_when: False
    changed_when: False
    when: destroy | bool
    with_items: "{{ exports }}"
    tags:
    - local_install

  - debug:
      var: item
    when: destroy | bool
    with_items: "{{ volume_stat_path.results }}"
    tags:
    - local_install

  - name: report mount points.  if previous command timed out, then it was a mount point that is causing problems and should also be removed
    debug:
      msg: "This is a mountpoint that will be removed"
    when: destroy and ( item.rc == 0 or item.rc == 124 )
    with_items: "{{ volume_stat_path.results }}"
    tags:
    - local_install

  - name: force unmount with shell when destroy is true.  bind must be removed before the hard mount - master.
    become: yes
    shell: |
      umount -l {{ item.item.path }}
    when: destroy and ( item.rc == 0 or item.rc == 124 )
    with_items: "{{ volume_stat_path.results }}"
    tags:
    - local_install

### End force removal of mounts

  - name: unmount when destroy is true for Lustre fsx master on workstation.  this will not check the fsx instance, and use the mounts originally defined in your ebs settings in secrets
    become: yes
    mount:
      path: "{{ item.path }}"
      state: unmounted
    when: destroy | bool
    with_items: "{{ exports }}"
    tags:
    - local_install

  - name: remove mounts from fstab when destroy is true for Lustre fsx master on workstation.  this will not check the fsx instance, and use the mounts originally defined in your ebs settings in secrets
    become: yes
    mount:
      path: "{{ item.path }}"
      state: absent
    when: destroy | bool
    with_items: "{{ exports }}"
    tags:
    - local_install


### cloud based mounts with ansible mount command

  - name: fsx ip
    debug:
      msg: "{{ fsx_ip }}"
    when: destroy == false
    tags:
    - cloud_install

  - name: insert/update block in in /etc/fstab on remote host for found exports using original unique pool and volume names - master
    become: yes
    mount:
      fstype: lustre
      path: "/{{ item.item.volume_name }}"
      opts: defaults,noatime,flock,_netdev 0 0
      # source must pull the ip from the fsx primary interface
      src: "{{ fsx_ip }}:tcp/{{ item.item.volume_name }}"
      state: "{{ ( item.item.path and item.rc == 0 and item.item.state == 'present') | ternary( 'mounted' , 'absent' ) }}"
    when: destroy == false
    with_items: "{{ fsx_exports }}"
    tags:
    - cloud_install

  - name: bind1 master mounts to named paths.  bind1 references the absolute mount names such as /cloud_prod.  bind1 is relative site names such as /prod, which are not pushed from cloud to onsite since those paths should exist onsite from a high performance local mount
    become: yes
    mount:
      fstype: none
      path: "{{ item.item.bind1 }}"
      opts: "x-systemd.requires=/{{ item.item.volume_name }},x-systemd.automount,bind,_netdev"
      src: "/{{ item.item.volume_name }}"
      # if the path exists, and it was found in the exports, then set to mounted, else remove.
      state: "{{ ( item.item.path and item.item.bind1 and item.rc == 0 and item.item.state == 'present' ) | ternary( 'mounted' , 'absent' ) }}"
    when: destroy == false
    with_items: "{{ fsx_exports }}"
    tags:
    - cloud_install

  - name: bind2 master mounts to named paths.  bind2 references the absolute mount names such as /cloud_prod.  bind1 is relative site names such as /prod, which are not pushed from cloud to onsite since those paths should exist onsite from a high performance local mount
    become: yes
    mount:
      fstype: none
      path: "{{ item.item.bind2 }}"
      opts: "x-systemd.requires=/{{ item.item.volume_name }},x-systemd.automount,bind,_netdev"
      src: "/{{ item.item.volume_name }}"
      # if the path exists, and it was found in the exports, then set to mounted, else remove.
      state: "{{ ( item.item.path and item.item.bind2 and item.rc == 0 and item.item.state == 'present' ) | ternary( 'mounted' , 'absent' ) }}"
    when: destroy == false
    with_items: "{{ fsx_exports }}"
    tags:
    - cloud_install

### check exports of onsite nas are available on cloud based host.  site mounts are never destroyed, since acces to onsite nas is assumed to exist at all times.
# - hosts: "{{ variable_host | default('role_node_centos') }}"
#   remote_user: "{{ variable_user | default('centos') }}"
#   gather_facts: "{{ variable_gather_facts | default('false') }}"
#   become: true
#   vars_files:
#     - /deployuser/ansible/group_vars/all/vars
#     - vars/main.yml

  # vars:
  #   variable_user: centos
  #   secrets_path: "{{ lookup('env','TF_VAR_secrets_path') }}"
  #   # ansible_become_pass: "{{ user_deadlineuser_pw }}"
  #   fsx_exports: "{{ hostvars[groups['role_fsx'][0]]['presence']['results'] }}"
  #   pcoip: False
  #   vars_files_locs: [ "/{{ secrets_path }}/{{ lookup('env','TF_VAR_envtier') }}/ebs-volumes/fsx_volumes.yaml", "files/fsx_volumes_{{ lookup('env','TF_VAR_envtier') }}.yaml", "files/fsx_volumes.yaml" ]

  # pre_tasks:
  # - name: show localnas1_private_ip
  #   debug:
  #     var: localnas1_private_ip
  #   tags:
  #   - always

  roles:
  - role: firehawkvfx.core.onsite_nfs_share
    when: localnas1_private_ip != 'none'