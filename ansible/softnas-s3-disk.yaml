- hosts: ansible_control
  remote_user: vagrant

  vars:
    # if pool name and volume name match an existing bucket, then it will be imported.  creation will skip.
    # commandline example to override defaults:
    # ansible-playbook -i ansible/inventory ansible/softnas-s3-disk.yaml --extra-vars "pool_name=pool0 volume_name=volume0 s3_disk_size_max_value=600 disk_device=0 encrypt_s3=true"
    # all disk_device numbers should be unique for any nas name.  /dev/s3- will be prepended to the disk_device number when mounted
    # volume names pools should both existing buckets match only if you intend to use raid.

    pool_name: pool0
    volume_name: volume0
    # shared pools should be enabled for DCHA.  caching is unavailable if this is used. tiering must be used insterad of caching for DCHA.
    share_pool: off
    disk_device: 0
    nas_name: softnas
    encrypt_s3: true
    s3_disk_size_max_value: 500
    s3_disk_size_max_units: GB
    s3_disk_region: "{{ aws_region }}"
    import_pool: true

    # this will be calculated later-
    existing_bucket: false
    existing_volume: false

  tasks:
  - set_fact:
      pool_name: "{{ pool_name }}"
      volume_name: "{{ volume_name }}"
      share_pool: "{{ share_pool }}"
      disk_device: "{{ disk_device }}"
      nas_name: "{{ nas_name }}"
      encrypt_s3: "{{ encrypt_s3 }}"
      s3_disk_size_max_value: "{{ s3_disk_size_max_value }}"
      s3_disk_size_max_units: "{{ s3_disk_size_max_units }}"
      s3_disk_region: "{{ s3_disk_region }}"
      existing_bucket: "{{ existing_bucket }}"
      existing_volume: "{{ existing_volume }}"

  - name: Create s3 extender disk. Query existing buckets.
    shell: |
      aws s3api list-buckets --query "Buckets[].Name"
    register: bucket_out

  - set_fact:
      buckets: "{{ bucket_out.stdout | from_json }}"
      search_string: "{{ public_domain | replace('.','') }}-{{ nas_name }}-{{ pool_name }}-{{ volume_name }}-{{ disk_device }}" 
      search_string_domain: "{{ public_domain | replace('.','') }}"
      search_string_device: "-{{ disk_device }}-"
      search_string_nas_name: "-{{ nas_name }}-"

  - debug:
      msg: "{{ buckets }}"



  - name: check for matching device - existing pool name, volume name, device name on the same nas name
    set_fact:
      existing_bucket: "{{ item }}"
    when:  search_string in item
    with_items:
      - "{{ buckets }}"

  - name: report no existing bucket found.  proceed to create new bucket.
    debug:
      msg: "report no existing bucket found.  proceed to create new bucket."
    when: existing_bucket == false

  - name: check existing disk_device for nas.
    fail:
      msg: |
        disk_device name already exists for this nas name in s3 bucket- "{{ item }}"
        Choose a different disk device name or nas name target to mount
    failed_when: (search_string_domain in item) and (search_string_nas_name in item) and (search_string_device in item) and (existing_bucket == false)
    with_items:
      - "{{ buckets }}"

  - name: existing_bucket
    debug:
      msg: "existing_bucket: {{ existing_bucket }}"
    when: existing_bucket != false

  - name: get bucket region if match found
    shell: |
      aws s3api get-bucket-location --bucket {{ existing_bucket }}
    register: bucket_location_shell_output
    when: existing_bucket != false

  - name: register bucket location
    set_fact:
      bucket_location: "{{ (bucket_location_shell_output.stdout|from_json).LocationConstraint }}"
    when: existing_bucket != false

  - name: Existing bucket with pool and volume name.  Will mount this to SoftNAS instead of creating a new bucket.
    debug:
      msg: "found bucket: {{ existing_bucket }}\nlocation: {{ bucket_location }}"
    when: existing_bucket != false

# create a new bucket
- hosts: role_softnas
  remote_user: centos
  become_user: root
  become: true

  vars:
    pool_name: "{{ hostvars['ansible_control']['pool_name'] }}"
    volume_name: "{{ hostvars['ansible_control']['volume_name'] }}"
    share_pool: "{{ hostvars['ansible_control']['share_pool'] }}"
    disk_device: "{{ hostvars['ansible_control']['disk_device'] }}"
    nas_name: "{{ hostvars['ansible_control']['nas_name'] }}"
    encrypt_s3: "{{ hostvars['ansible_control']['encrypt_s3'] }}"
    s3_disk_size_max_value: "{{ hostvars['ansible_control']['s3_disk_size_max_value'] }}"
    s3_disk_size_max_units: "{{ hostvars['ansible_control']['s3_disk_size_max_units'] }}"
    s3_disk_region: "{{ hostvars['ansible_control']['s3_disk_region'] }}"
    existing_bucket: "{{ hostvars['ansible_control']['existing_bucket'] }}"
    existing_volume: "{{ hostvars['ansible_control']['existing_volume'] }}"
  
  # import by bucket name if bucket name exists, then import, else create.
  # if created, then also:
  # create pool from disk mount
  # create volume from pool
  # ----
  # create nfs share.

  # create ability to delete
  tasks:
  
    - name: Ansible check file exists example.
      stat:
        path: /pool_name/volume_name/
      register: existing_volume_details

    - debug:
        msg: "volume already exists"
      when: existing_volume_details.stat.exists
    - set_fact:
        existing_volume: true
      when: existing_volume_details.stat.exists

    - debug:
        msg: "existing_bucket: {{ existing_bucket }}"
      when: existing_bucket == false

    - include_role: 
        name: softnas-s3-disk-create
      when: existing_bucket == false

# mount / import an existing bucket if pool and volume name match.  import pool.
- hosts: role_softnas
  remote_user: centos
  become_user: root
  become: true

  vars:
    pool_name: "{{ hostvars['ansible_control']['pool_name'] }}"
    volume_name: "{{ hostvars['ansible_control']['volume_name'] }}"
    share_pool: "{{ hostvars['ansible_control']['share_pool'] }}"
    disk_device: "{{ hostvars['ansible_control']['disk_device'] }}"
    nas_name: "{{ hostvars['ansible_control']['nas_name'] }}"
    encrypt_s3: "{{ hostvars['ansible_control']['encrypt_s3'] }}"
    s3_disk_size_max_value: "{{ hostvars['ansible_control']['s3_disk_size_max_value'] }}"
    s3_disk_size_max_units: "{{ hostvars['ansible_control']['s3_disk_size_max_units'] }}"
    s3_disk_region: "{{ hostvars['ansible_control']['s3_disk_region'] }}"
    existing_bucket: "{{ hostvars['ansible_control']['existing_bucket'] }}"
    bucket_location: "{{ hostvars['ansible_control']['bucket_location'] }}"
    # need to pull this data from the softnas instance, not defaults.
    existing_volume: "{{ hostvars['ansible_control']['existing_volume'] }}"
    import_pool: "{{ hostvars['ansible_control']['import_pool'] }}"
  
  # import by bucket name if bucket name exists, then import, else create.
  # if created, then also:
  # create pool from disk mount
  # create volume from pool
  # ----
  # create nfs share.

  # create ability to delete
  tasks:

    - debug:
        msg: "existing_bucket: {{ existing_bucket }}\nImporting."
      when: (existing_bucket != false) and (existing_volume == false)

    - include_role: 
        name: softnas-s3-disk-import
      when: (existing_bucket != false) and (existing_volume == false)

# after an import or creation of new disk, we need to setup the export.

- hosts: role_softnas
  remote_user: centos
  become_user: root
  become: true

  vars:
    pool_name: "{{ hostvars['ansible_control']['pool_name'] }}"
    volume_name: "{{ hostvars['ansible_control']['volume_name'] }}"
    share_pool: "{{ hostvars['ansible_control']['share_pool'] }}"
    disk_device: "{{ hostvars['ansible_control']['disk_device'] }}"
    nas_name: "{{ hostvars['ansible_control']['nas_name'] }}"
    encrypt_s3: "{{ hostvars['ansible_control']['encrypt_s3'] }}"
    s3_disk_size_max_value: "{{ hostvars['ansible_control']['s3_disk_size_max_value'] }}"
    s3_disk_size_max_units: "{{ hostvars['ansible_control']['s3_disk_size_max_units'] }}"
    s3_disk_region: "{{ hostvars['ansible_control']['s3_disk_region'] }}"
    existing_bucket: "{{ hostvars['ansible_control']['existing_bucket'] }}"
    bucket_location: "{{ hostvars['ansible_control']['bucket_location'] }}"
    import_pool: "{{ hostvars['ansible_control']['import_pool'] }}"
  
  # create nfs share.

  tasks:
  - name: set /etc/exports
    lineinfile:
      path: /etc/exports
      line: "# These mounts are managed in ansible playbook softnas-s3-disk.yaml and only one volume is exported currently for testing"
      backup: yes
    when: import_pool

  - name: ensure after a pool import that the path actually exists.  if so then exports will be updated
    stat:
      path: "/{{ pool_name }}/{{ volume_name }}/"
    register: volume_mount
    when: import_pool

  - debug:
      msg: "/{{ pool_name }}/{{ volume_name }}/ exists on softnas instance"
    when: volume_mount.stat.exists and volume_mount.stat.isdir and import_pool

  - name: insert/update block in in /etc/exports for volume
    blockinfile:
      path: /etc/exports
      block: |
        /{{ item.pool_name }}/{{ item.volume_name }}/ *(async,insecure,no_subtree_check,no_root_squash,rw,nohide)
      marker: "# {mark} ANSIBLE MANAGED BLOCK {{ item.volume_name }}"
    with_items:
      - { pool_name: "{{ pool_name }}", volume_name: "{{ volume_name }}" }
    when: volume_mount.stat.exists and volume_mount.stat.isdir and import_pool

  - name: update exports
    command: "exportfs -r -a"
    register: update_exports_output
    become: true
    when: volume_mount.stat.exists and volume_mount.stat.isdir and import_pool

  - debug:
      msg: "{{ update_exports_output.stdout }}"
    when: volume_mount.stat.exists and volume_mount.stat.isdir and import_pool